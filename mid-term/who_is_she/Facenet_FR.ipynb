{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: facenet-pytorch in /opt/conda/lib/python3.7/site-packages (2.5.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from facenet-pytorch) (1.16.5)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from facenet-pytorch) (2.21.0)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from facenet-pytorch) (0.11.1)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages (from facenet-pytorch) (6.2.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->facenet-pytorch) (2.8)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->facenet-pytorch) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->facenet-pytorch) (2019.9.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->facenet-pytorch) (3.0.4)\n",
      "Requirement already satisfied: torch==1.10.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->facenet-pytorch) (1.10.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.10.0->torchvision->facenet-pytorch) (4.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install facenet-pytorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from tqdm import tqdm \n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import mtcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from facenet_pytorch import InceptionResnetV1\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Running on device: {device}')\n",
    "\n",
    "model = InceptionResnetV1(pretrained='vggface2', dropout_prob=0.5, device=device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"/home/jovyan/riqdataset/aia-data2020/CNN_who_is_she/training_set\"\n",
    "test_data_path = \"/home/jovyan/riqdataset/aia-data2020/CNN_who_is_she/testing_set_retina_pick1\"\n",
    "\n",
    "img_size = 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(data_list):\n",
    "    data_img = []\n",
    "    for each in tqdm(data_list):\n",
    "        img = cv2.imread(each)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (img_size, img_size))\n",
    "        data_img.append(np.array(img))\n",
    "\n",
    "    return data_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_data_list = []\n",
    "y_data_list = []\n",
    "for roots, dirs, files in os.walk(train_data_path):\n",
    "    for each in files:\n",
    "        if each.find('checkpoint') == -1:\n",
    "            x_data_list.append(os.path.join(roots, each))\n",
    "            y_data_list.append(roots.split(\"/\")[-1])\n",
    "\n",
    "            \n",
    "testsetDD = pd.read_csv(\"/home/jovyan/riqdataset/aia-data2020/CNN_who_is_she/sample_submission.csv\")\n",
    "testsetDD['id'] = testsetDD['id'].astype(str) + '.png'\n",
    "#print(testsetDD['id'])\n",
    "fileNameOrder = testsetDD['id']\n",
    "\n",
    "test_x_data_list = []\n",
    "test_y_data_list = []\n",
    "for roots, dirs, files in os.walk(test_data_path):\n",
    "    for each in files:\n",
    "        if each.find('checkpoint') == -1:\n",
    "            test_x_data_list.append(os.path.join(roots, each))\n",
    "            test_y_data_list.append(roots.split(\"/\")[-1])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classname': 'classnum', 'rika': '0', 'risa': '1', 'yui': '2', 'akane': '3', 'neru': '4'}\n"
     ]
    }
   ],
   "source": [
    "class_path = \"/home/jovyan/riqdataset/aia-data2020/CNN_who_is_she/\"\n",
    "\n",
    "class_map = pd.read_csv(os.path.join(class_path, 'classmap.csv'),\n",
    "                        header=None, index_col=0)\n",
    "\n",
    "class_map[1]\n",
    "s = []\n",
    "\n",
    "class_map = class_map.to_dict()[1]\n",
    "print(class_map)\n",
    "\n",
    "s = list(class_map.keys())\n",
    "for x in s[1:]:\n",
    "    class_map[x] = int(class_map[x])\n",
    "class_map['classname'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>neru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>neru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>neru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>neru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>neru</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label\n",
       "0  neru\n",
       "1  neru\n",
       "2  neru\n",
       "3  neru\n",
       "4  neru\n",
       "5  neru\n",
       "6  neru\n",
       "7  neru\n",
       "8  neru\n",
       "9  neru"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data_list = pd.DataFrame(y_data_list, columns=['label'])\n",
    "y_data_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_class = len(y_data_list['label'].unique())\n",
    "num_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 4, 4, 4, 4, 4, 4, 4, 4])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data = y_data_list['label'].map(class_map).values.copy()\n",
    "y_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "facenet_input_size = (160,160)\n",
    "def extract_face(data_list):\n",
    "    data_img = []\n",
    "    cnt = 0\n",
    "    for filename in tqdm(data_list):\n",
    "        img = Image.open(filename)\n",
    "        img = img.convert('RGB')\n",
    "#         img = prewhiten(img)\n",
    "\n",
    "        w, h = img.size\n",
    "#         print(img,img.size[0], img.size[1])\n",
    "        img_array = np.array(img)\n",
    "        detector = mtcnn.MTCNN()\n",
    "        bounded_box = detector.detect_faces(img_array)\n",
    "        if len(bounded_box) != 0:\n",
    "            x1, y1, roiw, roih = bounded_box[0]['box']\n",
    "        else:\n",
    "            print('No.%d  can not detect face : %s ',( cnt , filename))\n",
    "#             face_image = np.zeros((160,160), dtype=np.int)\n",
    "            x1, y1, roiw, roih = 0, 0, w, h\n",
    "            print(x1, y1, roiw, roih)\n",
    "        x1, y1 = abs(x1), abs(y1)\n",
    "        x2, y2 = x1 + roiw, y1 + roih\n",
    "        face = img_array[y1:y2, x1:x2]\n",
    "        face_image = Image.fromarray(face)\n",
    "        face_image = face_image.resize(facenet_input_size)\n",
    "#         face_image = prewhiten(face_image)\n",
    "        cnt = cnt + 1\n",
    "        data_img.append(np.array(face_image))\n",
    "\n",
    "    return data_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 10/538 [00:27<32:12,  3.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.%d  can not detect face : %s  (9, '/home/jovyan/riqdataset/aia-data2020/CNN_who_is_she/training_set/neru/072.png')\n",
      "0 0 640 640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 11/538 [00:29<27:47,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.%d  can not detect face : %s  (10, '/home/jovyan/riqdataset/aia-data2020/CNN_who_is_she/training_set/neru/103.png')\n",
      "0 0 637 408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 42/538 [01:46<24:19,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.%d  can not detect face : %s  (41, '/home/jovyan/riqdataset/aia-data2020/CNN_who_is_she/training_set/neru/003.png')\n",
      "0 0 640 640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 88/538 [03:48<20:42,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.%d  can not detect face : %s  (87, '/home/jovyan/riqdataset/aia-data2020/CNN_who_is_she/training_set/neru/023.png')\n",
      "0 0 640 640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 98/538 [04:11<16:50,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.%d  can not detect face : %s  (97, '/home/jovyan/riqdataset/aia-data2020/CNN_who_is_she/training_set/neru/014.png')\n",
      "0 0 640 640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 124/538 [05:24<17:03,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.%d  can not detect face : %s  (123, '/home/jovyan/riqdataset/aia-data2020/CNN_who_is_she/training_set/yui/058.png')\n",
      "0 0 417 417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 142/538 [06:14<15:46,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.%d  can not detect face : %s  (141, '/home/jovyan/riqdataset/aia-data2020/CNN_who_is_she/training_set/yui/012.png')\n",
      "0 0 640 640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 146/538 [06:29<26:15,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.%d  can not detect face : %s  (145, '/home/jovyan/riqdataset/aia-data2020/CNN_who_is_she/training_set/yui/071.png')\n",
      "0 0 640 640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 192/538 [08:43<24:22,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.%d  can not detect face : %s  (191, '/home/jovyan/riqdataset/aia-data2020/CNN_who_is_she/training_set/yui/101.png')\n",
      "0 0 640 640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 198/538 [08:59<15:55,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.%d  can not detect face : %s  (197, '/home/jovyan/riqdataset/aia-data2020/CNN_who_is_she/training_set/yui/001.png')\n",
      "0 0 582 317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 210/538 [09:36<17:41,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.%d  can not detect face : %s  (209, '/home/jovyan/riqdataset/aia-data2020/CNN_who_is_she/training_set/yui/044.png')\n",
      "0 0 640 640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 272/538 [12:48<13:53,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.%d  can not detect face : %s  (271, '/home/jovyan/riqdataset/aia-data2020/CNN_who_is_she/training_set/risa/063.png')\n",
      "0 0 289 635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 370/538 [18:12<08:35,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.%d  can not detect face : %s  (369, '/home/jovyan/riqdataset/aia-data2020/CNN_who_is_she/training_set/akane/062.png')\n",
      "0 0 640 640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 440/538 [22:24<05:22,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.%d  can not detect face : %s  (439, '/home/jovyan/riqdataset/aia-data2020/CNN_who_is_she/training_set/rika/043.png')\n",
      "0 0 640 640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 461/538 [23:39<04:10,  3.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.%d  can not detect face : %s  (460, '/home/jovyan/riqdataset/aia-data2020/CNN_who_is_she/training_set/rika/087.png')\n",
      "0 0 640 640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 488/538 [25:22<02:51,  3.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.%d  can not detect face : %s  (487, '/home/jovyan/riqdataset/aia-data2020/CNN_who_is_she/training_set/rika/099.png')\n",
      "0 0 324 324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 505/538 [26:27<01:52,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.%d  can not detect face : %s  (504, '/home/jovyan/riqdataset/aia-data2020/CNN_who_is_she/training_set/rika/020.png')\n",
      "0 0 608 608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [28:34<00:00,  3.19s/it]\n"
     ]
    }
   ],
   "source": [
    "face_data_array = extract_face(x_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(538, 160, 160, 3)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(face_data_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 7/438 [00:25<25:59,  3.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.%d  can not detect face : %s  (6, '/home/jovyan/riqdataset/aia-data2020/CNN_who_is_she/testing_set_retina_pick1/8C920CE63A.png')\n",
      "0 0 1080 1080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 19/438 [01:15<26:03,  3.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.%d  can not detect face : %s  (18, '/home/jovyan/riqdataset/aia-data2020/CNN_who_is_she/testing_set_retina_pick1/29EC8E2C3E.png')\n",
      "0 0 1080 1080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 35/438 [02:21<26:10,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.%d  can not detect face : %s  (34, '/home/jovyan/riqdataset/aia-data2020/CNN_who_is_she/testing_set_retina_pick1/59DB41383B.png')\n",
      "0 0 1080 1080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 37/438 [02:27<24:15,  3.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.%d  can not detect face : %s  (36, '/home/jovyan/riqdataset/aia-data2020/CNN_who_is_she/testing_set_retina_pick1/E6C7BCED4B.png')\n",
      "0 0 1080 1080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 41/438 [02:42<24:09,  3.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.%d  can not detect face : %s  (40, '/home/jovyan/riqdataset/aia-data2020/CNN_who_is_she/testing_set_retina_pick1/697C12FB69.png')\n",
      "0 0 1080 1080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 45/438 [03:03<37:14,  5.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.%d  can not detect face : %s  (44, '/home/jovyan/riqdataset/aia-data2020/CNN_who_is_she/testing_set_retina_pick1/7000E8FB61.png')\n",
      "0 0 1080 1080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 51/438 [03:26<25:29,  3.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.%d  can not detect face : %s  (50, '/home/jovyan/riqdataset/aia-data2020/CNN_who_is_she/testing_set_retina_pick1/E1B2E2B632.png')\n",
      "0 0 1080 1080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 56/438 [03:44<22:49,  3.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.%d  can not detect face : %s  (55, '/home/jovyan/riqdataset/aia-data2020/CNN_who_is_she/testing_set_retina_pick1/DB9EDFE750.png')\n",
      "0 0 1080 1080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 58/438 [03:51<23:00,  3.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.%d  can not detect face : %s  (57, '/home/jovyan/riqdataset/aia-data2020/CNN_who_is_she/testing_set_retina_pick1/26CF244858.png')\n",
      "0 0 1080 1080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 78/438 [05:21<28:59,  4.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.%d  can not detect face : %s  (77, '/home/jovyan/riqdataset/aia-data2020/CNN_who_is_she/testing_set_retina_pick1/7042FD42EA.png')\n",
      "0 0 1080 1080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 84/438 [05:44<22:53,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.%d  can not detect face : %s  (83, '/home/jovyan/riqdataset/aia-data2020/CNN_who_is_she/testing_set_retina_pick1/45D7BB6117.png')\n",
      "0 0 1080 1080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 85/438 [05:47<21:46,  3.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.%d  can not detect face : %s  (84, '/home/jovyan/riqdataset/aia-data2020/CNN_who_is_she/testing_set_retina_pick1/730049E065.png')\n",
      "0 0 1080 1080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 91/438 [06:09<21:36,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.%d  can not detect face : %s  (90, '/home/jovyan/riqdataset/aia-data2020/CNN_who_is_she/testing_set_retina_pick1/4D4BC8FBBC.png')\n",
      "0 0 1080 1080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 102/438 [06:58<20:55,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.%d  can not detect face : %s  (101, '/home/jovyan/riqdataset/aia-data2020/CNN_who_is_she/testing_set_retina_pick1/AFE75C7A36.png')\n",
      "0 0 1080 1080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 103/438 [07:02<20:07,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.%d  can not detect face : %s  (102, '/home/jovyan/riqdataset/aia-data2020/CNN_who_is_she/testing_set_retina_pick1/A595081686.png')\n",
      "0 0 1080 1080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 116/438 [07:59<20:33,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.%d  can not detect face : %s  (115, '/home/jovyan/riqdataset/aia-data2020/CNN_who_is_she/testing_set_retina_pick1/B5844A85AD.png')\n",
      "0 0 1080 1080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 122/438 [08:22<20:16,  3.85s/it]Exception ignored in: <function ScopedTFGraph.__del__ at 0x7f7a4898e598>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/c_api_util.py\", line 48, in __del__\n",
      "    def __del__(self):\n",
      "KeyboardInterrupt\n",
      " 29%|██▉       | 129/438 [08:57<21:43,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.%d  can not detect face : %s  (128, '/home/jovyan/riqdataset/aia-data2020/CNN_who_is_she/testing_set_retina_pick1/A33867A445.png')\n",
      "0 0 1080 1080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 130/438 [09:01<21:05,  4.11s/it]"
     ]
    }
   ],
   "source": [
    "test_data_array = extract_face(test_x_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(test_data_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert face dataset to vector for facenet\n",
    "# def get_embedding(model, face_list):\n",
    "#     data_embedding =[]\n",
    "#     for face_pixels in face_list:\n",
    "#         face_pixels = face_pixels.astype('float32')\n",
    "#         face_pixels = (face_pixels - face_pixels.mean()) / face_pixels.std()\n",
    "#         samples = np.expand_dims(face_pixels, axis = 0)\n",
    "#         yhat = model.predict(samples)\n",
    "#         data_embedding.append(yhat[0])\n",
    "#     return data_embedding\n",
    "def get_embedding(model, face_pixels):\n",
    "    # scale pixel values\n",
    "    face_pixels = face_pixels.astype('float32')\n",
    "    # standardize pixel values across channels (global)\n",
    "    mean, std = face_pixels.mean(), face_pixels.std()\n",
    "    face_pixels = (face_pixels - mean) / std\n",
    "    # transform face into one sample\n",
    "    samples = np.expand_dims(face_pixels, axis=0)\n",
    "    # make prediction to get embedding\n",
    "    yhat = model.predict(samples)\n",
    "    return yhat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model, load_model\n",
    "\n",
    "model = load_model('facenet_keras.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import (Input, Dense, Dropout, Activation,\n",
    "                                     Flatten, BatchNormalization, Conv2D,\n",
    "                                     MaxPooling2D)\n",
    "\n",
    "from tensorflow.keras.applications.resnet_v2 import (ResNet101V2, preprocess_input)\n",
    "\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Dense, Activation, LeakyReLU, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_embedding_x = []\n",
    "test_embedding_x = []\n",
    "for train_x in face_data_array:\n",
    "    embedding_X = get_embedding(model, train_x)\n",
    "    data_embedding_x.append(embedding_X)\n",
    "    \n",
    "for test_x in test_data_array:\n",
    "    embedding_X = get_embedding(model, test_x)\n",
    "    test_embedding_x.append(embedding_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(data_embedding_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('traing_dataset.npz',  data_embedding_x,  y_data)\n",
    "np.savez_compressed('testing_dataset.npz',  test_embedding_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npz_trainging = np.load('traing_dataset.npz')\n",
    "data_embedding_x,  y_data = npz_trainging['arr_0'], npz_trainging['arr_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npz_testing = np.load('testing_dataset.npz')\n",
    "test_embedding_x = npz_testing['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_encoder = Normalizer(norm='l2')\n",
    "trainX = in_encoder.transform(data_embedding_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_encoder = LabelEncoder()\n",
    "out_encoder.fit(y_data)\n",
    "trainy = out_encoder.transform(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_encoder = Normalizer(norm='l2')\n",
    "testX = in_encoder.transform(test_embedding_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainy = shuffle(trainX, trainy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainX.shape, trainy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "svm_result_list = []\n",
    "for step in range(1):\n",
    "    model_svm = SVC(kernel='rbf', gamma = 0.5, probability = True)\n",
    "    model_svm.fit(trainX, trainy)\n",
    "    svm_result = model_svm.predict(testX)\n",
    "    svm_result_prob = model_svm.predict_proba(testX)\n",
    "    svm_result_list.append(svm_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDD = pd.read_csv(\"/home/jovyan/riqdataset/aia-data2020/CNN_who_is_she/sample_submission.csv\")\n",
    "AA = testDD.iloc[:,0]\n",
    "cnt = 9\n",
    "predResultHW = pd.DataFrame({'id':AA,'class':svm_result})\n",
    "predResultHW.to_csv(f\"submission{cnt}.csv\", index = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
